{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649631fb-809c-458e-9be2-9ca9df3c2541",
   "metadata": {},
   "outputs": [],
   "source": [
    " ! pip install -U bitsandbytes accelerate transformers datasets trl peft evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3307cb-8864-4e98-88c1-1a3af57aadf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "\n",
    "    AutoModelForCausalLM,\n",
    "\n",
    "    AutoModelForSequenceClassification,\n",
    "\n",
    "    AutoTokenizer,\n",
    "\n",
    "    BitsAndBytesConfig,\n",
    "\n",
    "    DistilBertTokenizer,\n",
    "\n",
    "    TrainingArguments,\n",
    "\n",
    "    pipeline,\n",
    "\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from trl import (\n",
    "\n",
    "    SFTTrainer,\n",
    "\n",
    "    PPOTrainer,\n",
    "\n",
    "    RewardTrainer,\n",
    "\n",
    "    PPOConfig,\n",
    "\n",
    "    RewardConfig,\n",
    "\n",
    "    AutoModelForCausalLMWithValueHead,\n",
    "\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from bitsandbytes.optim import AdamW8bit\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset as torchDataset\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639a69d-1ca7-4ede-acdc-c75fc053f8dc",
   "metadata": {},
   "source": [
    "# Hugging face and wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf6d2d1-0d07-4b2d-bd6b-1be4ae2dae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token='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')\n",
    "wandb.login(key=\"ba3349aecf7f23a3abb849de3155be527d3585f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07ec86-9c8c-4b2c-b570-d025986da4c8",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f55f8-e251-4f8a-bfdf-42dbb420e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"openai/summarize_from_feedback\", \"comparisons\")\n",
    "\n",
    "base_reward_model_checkpoint = \"google/gemma-2-2b\"\n",
    "\n",
    "reward_model_repo_name=\"reward_model\"\n",
    "\n",
    "reward_model_checkpoint=f\"JaishreeramCoder/{reward_model_repo_name}\"\n",
    "\n",
    "output_dir=\"/content/sample_data\"\n",
    "\n",
    "base_sft_model_checkpoint = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "sft_model_repo_name = \"sft_model\"\n",
    "\n",
    "sft_model_checkpoint=f\"JaishreeramCoder/{sft_model_repo_name}\"\n",
    "\n",
    "rlhf_model_repo_name=\"ppo_model\"\n",
    "\n",
    "rlhf_model_checkpoint=f\"JaishreeramCoder/{rlhf_model_repo_name}\"\n",
    "\n",
    "num_train_epochs_reward_model = 5\n",
    "\n",
    "num_train_epochs_sft = 5\n",
    "\n",
    "num_train_epochs_ppo_outer=5\n",
    "\n",
    "ppo_training_batch_size=8\n",
    "\n",
    "eval_batch_size = 8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb61b55-9606-4d29-a9ff-31f002cfb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187f865c-3ec5-4a39-83d1-be6af917e180",
   "metadata": {},
   "source": [
    "# RLHF based finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f95a78-a714-463e-8b6f-4bf63228db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "\n",
    "    load_in_4bit=True,\n",
    "\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "\n",
    ")\n",
    "\n",
    "lora_config =  LoraConfig(\n",
    "\n",
    "    lora_alpha=16,\n",
    "\n",
    "    lora_dropout=0.1,\n",
    "\n",
    "    r=64,\n",
    "\n",
    "    bias=\"none\",\n",
    "\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "\n",
    ")\n",
    "\n",
    "rlhf_tokenizer = AutoTokenizer.from_pretrained(sft_model_checkpoint)\n",
    "rlhf_tokenizer.pad_token = rlhf_tokenizer.eos_token\n",
    "\n",
    "rlhf_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    sft_model_checkpoint, quantization_config=quantization_config,peft_config=lora_config\n",
    ")\n",
    "rlhf_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64ab93-83ec-4580-8e0e-324827b270a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_fn(response):\n",
    "    \"\"\"\n",
    "    Takes single text as input and returns the reward score\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        reward_model.eval()\n",
    "        input_text = reward_tokenizer.decode(\n",
    "            response, skip_special_tokens=True\n",
    "        )  # skips eos, bos, pad token\n",
    "        input = reward_tokenizer(\n",
    "            input_text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        logits = reward_model(**input).logits\n",
    "        predicted_score = torch.tensor(logits.argmax(dim=-1),dtype=torch.float32)\n",
    "        return predicted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d285656-9e6b-4dad-9cc1-af353359817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rlhf_dataset(data):\n",
    "    input_ids, attention_mask = ([], [])\n",
    "    for i in range(len(data[\"choice\"])):\n",
    "        input = f\"Summarize the following text:\\n\\n{data['info'][i]['post']}\"\n",
    "        cur = rlhf_tokenizer(\n",
    "            input,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding_side=\"left\",\n",
    "        )\n",
    "        cur_input_ids = cur.input_ids\n",
    "        cur_attention_mask = cur.attention_mask\n",
    "        input_ids.append(cur_input_ids)\n",
    "        attention_mask.append(cur_attention_mask)\n",
    "    output = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "    output = Dataset.from_dict(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f358eb-ad07-44d0-97d8-7cd1fa79c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlhf_train_dataset = get_rlhf_dataset(dataset[\"train\"][2000:3000])\n",
    "rlhf_eval_dataset = get_rlhf_dataset(dataset[\"validation\"][2000:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e7131-eaff-4a23-b5b1-149428bf060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "\n",
    "    \"min_length\": -1,  # don't ignore the EOS token\n",
    "\n",
    "    \"top_k\": 0.0,  # no top-k sampling\n",
    "\n",
    "    \"top_p\": 1.0,  # no nucleus sampling\n",
    "\n",
    "    \"do_sample\": True,  # yes, we want to sample\n",
    "\n",
    "    \"eos_token_id\": rlhf_tokenizer.eos_token_id,\n",
    "\n",
    "    \"bos_token_id\": rlhf_tokenizer.bos_token_id,\n",
    "\n",
    "    \"pad_token_id\": rlhf_tokenizer.eos_token_id,  # most decoder models don't have a padding token - use EOS token instead\n",
    "\n",
    "    \"max_new_tokens\": 32,  # specify how many tokens you want to generate at most\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2437b-f836-4a97-8b84-d7de2bd0dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, reward_model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        reward_model.eval()\n",
    "        reward_value = []\n",
    "        data_size = len(data[\"input_ids\"])\n",
    "        \n",
    "        for i in tqdm(range(0, data_size, eval_batch_size)):\n",
    "            cur_data = torch.tensor(data[\"input_ids\"][i : i + eval_batch_size])\n",
    "            cur_data=cur_data.detach().clone()\n",
    "            cur_data=cur_data.to(device)\n",
    "            response = model.generate(cur_data, **generation_kwargs)\n",
    "            response = response[: cur_data.shape[1] :]\n",
    "            for j in range(eval_batch_size):\n",
    "                reward_value.append(reward_fn(response[j]))\n",
    "        avg_reward = np.mean(np.array(reward_value))\n",
    "        return avg_reward\n",
    "\n",
    "sft_avg_reward = evaluate(ppo_model, rlhf_eval_dataset, reward_model)\n",
    "print(f\"Average Reward for supervised finetuned model: {sft_avg_reward}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    cur_data = torch.tensor(rlhf_eval_dataset[\"input_ids\"][0])\n",
    "    cur_data=cur_data.detach().clone()\n",
    "    cur_data=cur_data.to(device)\n",
    "    response = rlhf_model.generate(cur_data, **generation_kwargs)\n",
    "    input_text=rlhf_tokenizer.decode(cur_data)\n",
    "    sft_response_text=rlhf_tokenizer.decode(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0eb1c5-91f8-4f8c-9ccc-bf1b9617c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_parameters(rlhf_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21913268-74a0-491f-a014-abe3e428724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = PPOConfig(\n",
    "\n",
    "    gradient_accumulation_steps=ppo_training_batch_size,\n",
    "\n",
    "    batch_size=ppo_training_batch_size,\n",
    "\n",
    "    mini_batch_size=1,\n",
    "\n",
    "    learning_rate=1e-5,\n",
    "\n",
    "    model_name=rlhf_model_checkpoint,\n",
    "\n",
    "    is_peft_model=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d653448-b510-473a-96d9-2395b16674cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(num_train_epochs_ppo_outer)):\n",
    "    for i in range(0,len(rlhf_eval_dataset),ppo_training_batch_size):\n",
    "        reward_value=[]\n",
    "        data=torch.tensor(rlhf_eval_dataset[\"input_ids\"][i:i+ppo_training_batch_size])\n",
    "        data=data.detach().clone()\n",
    "        data=data.to(device)\n",
    "        data=[data[j] for j in range(ppo_training_batch_size)]\n",
    "        responses=ppo_trainer.generate(data,**generation_kwargs,return_prompt=False)\n",
    "        for j in range(ppo_training_batch_size):\n",
    "            cur_reward=reward_fn(responses[j])\n",
    "            reward_value.append(cur_reward)\n",
    "        ppo_trainer.step(queries=data,responses=responses,scores=reward_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e133753-32c0-4ed2-9eee-7ba994f0ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    cur_data = torch.tensor(rlhf_eval_dataset[\"input_ids\"][0])\n",
    "    cur_data=cur_data.detach().clone()\n",
    "    cur_data=cur_data.to(device)\n",
    "    response = rlhf_model.generate(cur_data, **generation_kwargs)\n",
    "    input_text=rlhf_tokenizer.decode(cur_data)\n",
    "    rlhf_response_text=rlhf_tokenizer.decode(response)\n",
    "    print(f\"Input Text:\\n{input_text}\")\n",
    "    print(f\"\\nSFT model Response:\\n{sft_response_text}\")\n",
    "    print(f\"\\nRLHF model Response:\\n{rlhf_response_text}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40547cc-170e-4e3f-8db8-27398f183c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlhf_avg_reward = evaluate(ppo_trainer.model, rlhf_eval_dataset, reward_model)\n",
    "print(f\"Average Reward for Supervised finetuned model: {sft_avg_reward}\")\n",
    "print(f\"Average Reward for RLHF finetuned model: {rlhf_avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece530e6-a8dd-4e5d-8d82-18134e921c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(decoded_preds, decoded_actual_labels):\n",
    "\n",
    "    result = rouge_metric.compute(\n",
    "\n",
    "        predictions=decoded_preds, references=decoded_actual_labels\n",
    "\n",
    "    )\n",
    "\n",
    "    print(f\"RLHF Model ROUGE values: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f14c4b-4cba-4548-a145-6fc2d16eedb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sft_model(sft_model, sft_eval_dataset):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        sft_model.eval()\n",
    "\n",
    "        decoded_preds = []\n",
    "\n",
    "        decoded_actual_labels = []\n",
    "\n",
    "        for i in tqdm(range(0, len(sft_eval_dataset[\"input_ids\"]), eval_batch_size)):\n",
    "\n",
    "            cur_data = torch.tensor(\n",
    "\n",
    "                sft_eval_dataset[\"input_ids\"][i : i + eval_batch_size]\n",
    "\n",
    "            )\n",
    "\n",
    "            cur_data=cur_data.to(device)\n",
    "\n",
    "            cur_preds = sft_model.generate(cur_data, **generation_kwargs)\n",
    "\n",
    "            cur_preds = cur_preds[:, cur_data.shape[1] :]\n",
    "\n",
    "            for j in range(eval_batch_size):\n",
    "\n",
    "                generated_text = sft_tokenizer.decode(\n",
    "\n",
    "                    cur_preds[j], skip_special_tokens=True\n",
    "\n",
    "                )\n",
    "\n",
    "                decoded_preds.append(generated_text)\n",
    "\n",
    "            cur_actual_label_ids = torch.tensor(\n",
    "\n",
    "                sft_eval_dataset[\"labels\"][i : i + eval_batch_size]\n",
    "\n",
    "            )\n",
    "\n",
    "            for j in range(eval_batch_size):\n",
    "\n",
    "                decoded_actual_labels.append(\n",
    "\n",
    "                    sft_tokenizer.decode(\n",
    "\n",
    "                        cur_actual_label_ids[j], skip_special_tokens=True\n",
    "\n",
    "                    )\n",
    "\n",
    "                )\n",
    "\n",
    "        sft_model_eval_result = compute_metrics(\n",
    "\n",
    "            decoded_preds=decoded_preds, decoded_actual_labels=decoded_actual_labels\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "evaluate_sft_model(rlhf_model, rlhf_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7c171-1a44-40d0-910a-f687e5e39595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rlhf_model.merge_and_unload()\n",
    "rlhf_model.push_to_hub(rlhf_model_repo_name)\n",
    "rlhf_tokenizer.push_to_hub(rlhf_model_repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d4448-081f-4234-8da0-592bd9a1a530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
