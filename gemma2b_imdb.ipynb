{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db1c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq transformers datasets bitsandbytes accelerate scikit-learn peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb1bd029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GemmaTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "TOKEN = \"<TOKEN>\"\n",
    "MODEL_NAME = \"google/gemma-2b\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e94ad82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52aea9be26da4b92b9fa73b970368c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\", token=TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90552146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "\n",
      "    features: ['text', 'label'],\n",
      "\n",
      "    num_rows: 25000\n",
      "\n",
      "})\n",
      "\n",
      "Dataset({\n",
      "\n",
      "    features: ['text', 'label'],\n",
      "\n",
      "    num_rows: 25000\n",
      "\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('imdb')\n",
    "# dataset = dataset.shuffle().train_test_split(0.1, stratify_by_column=\"label\")\n",
    "train_ds = dataset['train']\n",
    "test_ds = dataset['test'].shuffle()\n",
    "print(train_ds)\n",
    "print(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80a7eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = {\n",
    "    \" Positive\": 40695,\n",
    "    \" Negative\": 48314,\n",
    "    \" positive\": 6222,\n",
    "    \" negative\": 8322,\n",
    "    \"Positive\": 35202,\n",
    "    \"Negative\": 39654,\n",
    "}\n",
    "\n",
    "def get_prompt_list(item):\n",
    "    res = []\n",
    "    for text, label in zip(item['text'], item['label']):\n",
    "        content = get_prompt(text)\n",
    "        content += \"Positive\" if label == 1 else 'Negative'\n",
    "        res.append(content)\n",
    "    # print(res)\n",
    "    return res\n",
    "\n",
    "def get_prompt(query):\n",
    "    content = f\"\"\"### REVIEW:\n",
    "{query}\n",
    "\n",
    "### SENTIMENT:\n",
    "\"\"\"\n",
    "    return content\n",
    "\n",
    "def llm(query):\n",
    "    prompt = get_prompt(query)\n",
    "    inputs = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=1, output_scores=True, return_dict_in_generate=True)\n",
    "\n",
    "    positive_pred = outputs.scores[0][0][TOKENS['Positive']]\n",
    "    negative_pred = outputs.scores[0][0][TOKENS['Negative']]\n",
    "\n",
    "    positive_pred = positive_pred.cpu()\n",
    "    negative_pred = negative_pred.cpu()\n",
    "\n",
    "    scores = np.array([positive_pred, negative_pred])\n",
    "    probs = np.exp(scores) / np.sum(np.exp(scores))\n",
    "    \n",
    "    positive_prob = probs[0]\n",
    "    negative_prob = probs[1]\n",
    "    # print(positive_pred, negative_pred)\n",
    "    \n",
    "    return tokenizer.decode(outputs.sequences[0]), positive_prob\n",
    "\n",
    "def predict(query, print_res = False):\n",
    "    text, prob = llm(query)\n",
    "    if print_res:\n",
    "        print(text)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b82231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45649707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:22<00:00,  6.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before fine tuning\n",
      "\n",
      "Accuracy: 0.772\n",
      "\n",
      "Precision: 0.7005813953488372\n",
      "\n",
      "Recall: 0.9563492063492064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "y_pred = []\n",
    "y_test = []\n",
    "i = 0\n",
    "for ex in tqdm(test_ds):\n",
    "    pred, label = predict(ex['text']), ex['label']\n",
    "    # print(pred, label)\n",
    "    y_pred.append(pred)\n",
    "    y_test.append(label)\n",
    "\n",
    "print('before fine tuning')\n",
    "print_metrics(y_test, np.round(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e79e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80bcfae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "\n",
      "  warnings.warn(\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "\n",
      "  warnings.warn(\n",
      "\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3124' max='3124' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3124/3124 1:16:09, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.087200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.957400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.925700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.849600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.859500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.900100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.801900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.826700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.808100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.759400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.737200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.799300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.771000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.789000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.744500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.749100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.757900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.762500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.748100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.736300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.733500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.781300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.710700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.753300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.753700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.712700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.781600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.765800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.729100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.725200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.762200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.724000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.755700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.733700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.786900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.771200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.749000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.732400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.751300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.768600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.749400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.738400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.746700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.733500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.807500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.764700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.759300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.716700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.783100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>2.733500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>2.735200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>2.782400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>2.719900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.731800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>2.756500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>2.751800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>2.718400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>2.725200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.725800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>2.757800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>2.711400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>2.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>2.738300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.735800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>2.729900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>2.728600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>2.685700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>2.735500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.729900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>2.724000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>2.762900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>2.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>2.704600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.754500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>2.709600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>2.755400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>2.715300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>2.718100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>2.690200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>2.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>2.733100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>2.721800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.719200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>2.772900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>2.720400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>2.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>2.707800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.709500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>2.732600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>2.745300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>2.743800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>2.766600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.752300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>2.735700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>2.705900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>2.749300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>2.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.714800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>2.755400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>2.724200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>2.758300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>2.709700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.752000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>2.719500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>2.735300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>2.745800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>2.714200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.757100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>2.715900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>2.734900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>2.762400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>2.749400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.694300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>2.748700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>2.747300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>2.738000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>2.694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.752800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>2.733900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>2.736500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>2.717400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>2.743800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.696900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>2.721000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>2.694100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>2.698400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>2.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.725100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>2.729800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>2.748400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>2.728700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>2.752500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>2.752500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>2.719600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>2.711200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>2.729600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.723700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>2.703600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>2.689700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>2.733200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>2.697500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.745500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>2.729500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>2.739400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>2.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>2.758100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.713800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>2.740400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-65eb8e55-0e6e4f07715cd2f9014cc9ad;0a375945-c0c5-43c0-b71e-983fa042af08)\n",
      "\n",
      "\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2b/resolve/main/config.json.\n",
      "\n",
      "Repo model google/gemma-2b is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-2b.\n",
      "\n",
      "  warnings.warn(\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-2b - will assume that the vocabulary was not modified.\n",
      "\n",
      "  warnings.warn(\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-65eb9125-441b933b448f8c3e4ae075af;02dea605-588c-4896-b6ca-b158ece08461)\n",
      "\n",
      "\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2b/resolve/main/config.json.\n",
      "\n",
      "Repo model google/gemma-2b is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-2b.\n",
      "\n",
      "  warnings.warn(\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-2b - will assume that the vocabulary was not modified.\n",
      "\n",
      "  warnings.warn(\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-65eb9407-6705678242def98512e7f7b2;87a184a1-ee9e-429e-87f7-af6d2fd8bf60)\n",
      "\n",
      "\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2b/resolve/main/config.json.\n",
      "\n",
      "Repo model google/gemma-2b is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-2b.\n",
      "\n",
      "  warnings.warn(\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-2b - will assume that the vocabulary was not modified.\n",
      "\n",
      "  warnings.warn(\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-65eb96e3-5853af520ec4015873e29675;56b510a8-89a7-485c-bc50-b56b3b1e2be2)\n",
      "\n",
      "\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2b/resolve/main/config.json.\n",
      "\n",
      "Repo model google/gemma-2b is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-2b.\n",
      "\n",
      "  warnings.warn(\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-2b - will assume that the vocabulary was not modified.\n",
      "\n",
      "  warnings.warn(\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-65eb99be-65368da26097e5433973005e;58b2ca2e-89f4-4fc4-ab21-92a3f7fdc49c)\n",
      "\n",
      "\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2b/resolve/main/config.json.\n",
      "\n",
      "Repo model google/gemma-2b is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-2b.\n",
      "\n",
      "  warnings.warn(\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-2b - will assume that the vocabulary was not modified.\n",
      "\n",
      "  warnings.warn(\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-65eb9c98-3b5f833f0a56196563a790b7;afd307cc-7e87-4d68-9b35-2c88b2996b91)\n",
      "\n",
      "\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/google/gemma-2b/resolve/main/config.json.\n",
      "\n",
      "Repo model google/gemma-2b is gated. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in google/gemma-2b.\n",
      "\n",
      "  warnings.warn(\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in google/gemma-2b - will assume that the vocabulary was not modified.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3124, training_loss=2.7479343209773415, metrics={'train_runtime': 4571.8426, 'train_samples_per_second': 10.937, 'train_steps_per_second': 0.683, 'total_flos': 3.188944477846733e+17, 'train_loss': 2.7479343209773415, 'epoch': 2.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=2,\n",
    "        fp16=True,\n",
    "        logging_steps=20,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        report_to=\"none\"\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=get_prompt_list,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bc88187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [31:45<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after fine tuning\n",
      "\n",
      "Accuracy: 0.96608\n",
      "\n",
      "Precision: 0.9623075702269481\n",
      "\n",
      "Recall: 0.97016\n"
     ]
    }
   ],
   "source": [
    "# this is very slow if you want to run on all 25k samples :)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "y_pred = []\n",
    "y_test = []\n",
    "i = 0\n",
    "for ex in tqdm(test_ds):\n",
    "    pred, label = predict(ex['text']), ex['label']\n",
    "    y_pred.append(pred)\n",
    "    y_test.append(label)\n",
    "\n",
    "print('after fine tuning')\n",
    "print_metrics(y_test, np.round(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f97b9693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>### REVIEW:\n",
      "\n",
      "The EMPEROR'S NEW GROOVE cast returns for Disney Pictures follow up, but this time the spotlight is on Kronk(voiced by Patrick Warburton), who is no longer Yzma's(Eartha Kitt)henchman. Kronk has started a new life and is very happy with his role as chef of his own restaurant. Things go merrily along until Kronk gets word that his Papi(John Mahoney)is coming for a visit. Kronk is worried, because he knows that his life won't impress his Papi. One thing that he has always wanted and never received is a \"thumbs up\" from his dad. A flurry of blunders and a gigantic cheese explosion in the restaurant leaves our likable hero very deep in trouble and anxiety. To save the day, a little help from his friends.<br /><br />Other voices: Tracey Ullman, David Spade, John Goodman, Wendie Malick, April Winchell and Gatlin Green.\n",
      "\n",
      "\n",
      "\n",
      "### SENTIMENT:\n",
      "\n",
      "Positive\n",
      "\n",
      "\n",
      "\n",
      "### SENTIMENT\n",
      "\n",
      "0.6853661 0\n",
      "\n",
      "<bos>### REVIEW:\n",
      "\n",
      "**SPOILERS AHEAD**<br /><br />It is really unfortunate that a movie so well produced turns out to be<br /><br />such a disappointment. I thought this was full of (silly) clichés and<br /><br />that it basically tried to hard. <br /><br />To the (American) guys out there: how many of you spend your<br /><br />time jumping on your girlfriend's bed and making monkey<br /><br />sounds? To the (married) girls: how many of you have suddenly<br /><br />gone from prudes to nymphos overnight--but not with your<br /><br />husband? To the French: would you really ask about someone<br /><br />being \"à la fac\" when you know they don't speak French? Wouldn't<br /><br />you use a more common word like \"université\"? <br /><br />I lived in France for a while and I sort of do know and understand<br /><br />Europe (and I love it), but my (German) roommate and I found this<br /><br />pretty insulting overall. It looked like a movie funded by the<br /><br />European Parliament, and it tried too hard basically. It had all<br /><br />sorts of differences that it tried to tie together (not a bad thing in<br /><br />itself) but the result is at best awkward, but in fact ridiculous--too<br /><br />many clashes that wouldn't really happen. Then the end of the<br /><br />movie--the last 10 minutes--ruined all the rest. Why doesn't Xavier<br /><br />talk to the Erasmus students he meets back in Paris? Why does<br /><br />he just walk off? Why does he just run away from his job, is that<br /><br />\"freedom\"? And in the end, is the new Europe supposed to rest on<br /><br />a bunch of people who smoke up and shag all day? Is this what<br /><br />it's made up of? <br /><br />Besides, the acting was pretty horrible. I can't believe Judith<br /><br />Godrèche's role and acting. Why was she made to look like<br /><br />Emanuelle Béart so much? At first I thought Xavier was OK but<br /><br />with retrospect I think he was pretty bad. <br /><br />And that's all really too bad, because technically (opening credits,<br /><br />scenes when he's asking what papers he needs) it was really<br /><br />good (except for sound editing around the British siblings), and the<br /><br />soundtrack was great too. So the form was good, but the content<br /><br />pretty horrible.\n",
      "\n",
      "\n",
      "\n",
      "### SENTIMENT:\n",
      "\n",
      "Negative\n",
      "\n",
      "\n",
      "\n",
      "### SENTIMENT\n",
      "\n",
      "0.0052424683 1\n",
      "\n",
      "<bos>### REVIEW:\n",
      "\n",
      "This movie wasn't too funny. It wasn't too horrible either. Just a fun ride aboard the Soul Plane. It's a black-owned flight service with a kick-ass disco, beautiful bar, and pumping stewards. The plot takes you through the whole idea of a black (not race or color, but 'style') -owned plane with the same style of black humor. At times, there are some great rare scenes like when the main character uses the 'normal' lavatory in the 'normal' plane. Check that out. There are lulls, however, and I find it most in the character who played the Soul Pilot. Watch to find out who it is. All in all, it's a fun movie. Tom Arnold adds to the fun with his 'cracker' attitude. This is one movie to watch when playing poker, when you're drunk, or just vegging.\n",
      "\n",
      "\n",
      "\n",
      "### SENTIMENT:\n",
      "\n",
      "Positive\n",
      "\n",
      "\n",
      "\n",
      "### IN THE\n",
      "\n",
      "0.82601774 0\n",
      "\n",
      "<bos>### REVIEW:\n",
      "\n",
      "The 3rd in the series finds Paul Kersey (Bronson) turning vigilante to get revenge on the thugs that murdered his old buddy. I don't know why this movie shoved me into it, but somehow it did. I found myself rooting for Bronson to wipe the floor with those punks. Every time he blew one of them away I felt good. This movie does not take itself seriously, but what if it did? There is a good build-up to the fireworks finale in which Bronson goes on a rampage. But as far as acting and plot go, it just doesn't measure up. If I lived in that neighborhood, I would get out as fast as I could, but it seems like the people are asking for trouble. I know there is that mentality that we need to save our streets, but there is a limit here folks. I had to give it a 4. Sure there are good \"blow 'em away\" scenes but that's about it. At that time, Bronson was 64. I'm sure if those thugs really wanted to they could have their way with Bronson. Bronson takes the place of a Schwarzanegger or Stallone in this movie. This movie gives you a sense of rejoice. The common man can save the neighborhood, save the day. To sum it up, this is far from being the original Death Wish, but it is rather good if you are just looking for an hour and a half of shoot 'em up.\n",
      "\n",
      "\n",
      "\n",
      "### SENTIMENT:\n",
      "\n",
      "Positive\n",
      "\n",
      "\n",
      "\n",
      "### SENTIMENT\n",
      "\n",
      "0.57183105 0\n",
      "\n",
      "<bos>### REVIEW:\n",
      "\n",
      "It's exactly what the title tells you...an island inhabited by fishmen. Shipwrecked doctor Claudio Cassinelli and crew land on the island, they're either picked off by the fishmen or roped into working for treasure hunting lunatic Richard Johnson. Cassinelli discovers that Johnson, who believes he's found the lost city of Atlantis, has been keeping disgraced scientist Joseph Cotten and his daughter Barbara Bach hostage for 15 years so the fishmen can uncover a treasure trove beneath the sea. Cotten, of course, is a complete madman. Bach and Cassinelli have great chemistry. This insanity was directed by Sergio Martino and is not, surprisingly, without merit. It's fast paced, reasonably well acted and the fishmen look pretty convincing (though it's unlikely anyone could prove that these things DON'T look like actual fishmen). There's an excellent music score by Luciano Michelini.\n",
      "\n",
      "\n",
      "\n",
      "### SENTIMENT:\n",
      "\n",
      "Positive\n",
      "\n",
      "\n",
      "\n",
      "### SENTIMENT\n",
      "\n",
      "0.96587044 0\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in range(len(y_pred)):\n",
    "    if np.round(y_pred[i]) != y_test[i]:\n",
    "        example = test_ds[i]\n",
    "        print(predict(example['text'], print_res=True), example['label'])\n",
    "        cnt += 1\n",
    "        if cnt == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9898e78c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
